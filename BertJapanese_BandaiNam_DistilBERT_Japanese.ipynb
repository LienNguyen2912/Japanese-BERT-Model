{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x4lDW8X-tsVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37837abd-b65f-4718-b026-0906a90f8051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB5bXp4Ou5Ec",
        "outputId": "af641fb6-140f-4e7c-bfac-a5f4a9d0270b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brMGHqYqu8Q3",
        "outputId": "6f192fbd-a204-4876-d1eb-14114ab33f0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 24 01:08:42 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0    30W /  70W |   2245MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMqubx6NvBUZ",
        "outputId": "12ad5f92-30ef-4bb4-bed3-5ebf31cccc2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fugashi in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: unidic-lite in /usr/local/lib/python3.10/dist-packages (1.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download DistilBERT-base-jp.zip, unzip it, then create a folder called DistilBERT containinng 3 files below:\n",
        "*   config.json\n",
        "*   pytorch_model.bin\n",
        "*   vocab.txt"
      ],
      "metadata": {
        "id": "K3pXlreACs-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download DistilBERT-base-jp\n",
        "!wget -O file.zip \"https://github.com/BandaiNamcoResearchInc/DistilBERT-base-jp/releases/download/1.0/DistilBERT-base-jp.zip\"\n",
        "# Step 2: Unzip the file\n",
        "!unzip file.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ci5of717adS",
        "outputId": "5b48654f-6c7a-461a-e040-3635b03c1e23"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-24 01:54:23--  https://github.com/BandaiNamcoResearchInc/DistilBERT-base-jp/releases/download/1.0/DistilBERT-base-jp.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/257822283/23566400-84b6-11ea-9e58-c5b9a64a9f2d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231124T015424Z&X-Amz-Expires=300&X-Amz-Signature=fae779489c42b72b194c8f94bc4c5913e8f49463f1000eb8cd8a468bea8b047e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=257822283&response-content-disposition=attachment%3B%20filename%3DDistilBERT-base-jp.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-24 01:54:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/257822283/23566400-84b6-11ea-9e58-c5b9a64a9f2d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231124T015424Z&X-Amz-Expires=300&X-Amz-Signature=fae779489c42b72b194c8f94bc4c5913e8f49463f1000eb8cd8a468bea8b047e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=257822283&response-content-disposition=attachment%3B%20filename%3DDistilBERT-base-jp.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252055644 (240M) [application/octet-stream]\n",
            "Saving to: ‘file.zip’\n",
            "\n",
            "file.zip            100%[===================>] 240.38M   316MB/s    in 0.8s    \n",
            "\n",
            "2023-11-24 01:54:25 (316 MB/s) - ‘file.zip’ saved [252055644/252055644]\n",
            "\n",
            "Archive:  file.zip\n",
            "replace DistilBERT-base-jp/config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: DistilBERT-base-jp/config.json  \n",
            "replace DistilBERT-base-jp/docs/GUIDE.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: DistilBERT-base-jp/docs/GUIDE.md  \n",
            "replace DistilBERT-base-jp/pytorch_model.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: DistilBERT-base-jp/pytorch_model.bin  \n",
            "replace DistilBERT-base-jp/README.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: DistilBERT-base-jp/README.md  \n",
            "replace DistilBERT-base-jp/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: DistilBERT-base-jp/vocab.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipadic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tamHASfBB9KG",
        "outputId": "e6d77743-7b6c-427f-a907-53259cd7c660"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=60b7989a43083eca9fd91b2b6657428ba32c0ed2b8842dbd6b9064740766ed56\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic\n",
            "Successfully installed ipadic-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read from local path\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "LOCAL_PATH = \"/content/DistilBERT-base-jp/DistilBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
        "model = AutoModel.from_pretrained(LOCAL_PATH)"
      ],
      "metadata": {
        "id": "7dtvXP_ZBQ9g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# Set the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print (device)\n",
        "\n",
        "# Define a function to classify a text and get CLS's embedding\n",
        "def get_cls_embedding_and_tokenized_output(input_corpus):\n",
        "    # Tokenize input and get token outputs\n",
        "    encoded_input_corpus  = tokenizer(input_corpus, return_tensors=\"pt\", truncation=True)\n",
        "    # The DistilBertModel doesn't require the token_type_ids argument, so it is required to remove token_type_ids from the encoded_input_corpus dictionary before passing it to the forward method.\n",
        "    # If not, \"TypeError: DistilBertModel.forward() got an unexpected keyword argument 'token_type_ids'\"  will occur\n",
        "    # Remove the 'token_type_ids' key from the dictionary\n",
        "    encoded_input_corpus.pop('token_type_ids', None)\n",
        "    encoded_input_corpus = {key: val.to(device) for key, val in encoded_input_corpus.items()}\n",
        "\n",
        "    # Run the model and get CLS's embedding\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        output  = model(**encoded_input_corpus )\n",
        "        cls_embedding = output.last_hidden_state[:, 0]\n",
        "        # Convert the tokenized output to a list of tokens\n",
        "        tokenized_output = tokenizer.convert_ids_to_tokens(encoded_input_corpus[\"input_ids\"].squeeze().cpu().numpy())\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return cls_embedding, tokenized_output, elapsed_time, encoded_input_corpus\n",
        "\n",
        "#input_corpus = \"これは日本語のテキストのコープスです。\"\n",
        "input_corpus = \"\"\"\n",
        "[オペレーター] : こんにちは！お電話ありがとうございます。私の名前はエミリーで、今日はお手伝いできて嬉しいです。どのようにお手伝いできますか？\n",
        "[ユーザー] : こんにちは、エミリー。貴社のサービスに興味がありますが、それが私にとって適しているかどうか分かりません。\n",
        "[オペレーター] : 完全に理解していますし、興味を持っていただきありがとうございます！様々なニーズに対応した幅広いサービスを提供しています。サービスに関してもう少し教えていただけますか？\n",
        "[ユーザー] : そうですね、信頼性があり、手頃な価格のものが必要です。過去にサービスで悪い経験をしました。\n",
        "[オペレーター] : 了解しました。当社はすべてのサービスで信頼性と手頃な価格を重視しています。パッケージについての詳細を共有させていただきます。現在、新規顧客向けに割引料金が適用される特別なプロモーションを実施しています。これにより、高品質なサービスを手頃な価格でご利用いただけます。\n",
        "[ユーザー] : 興味深いですね。サービスの具体的な機能について教えていただけますか？\n",
        "[オペレーター] : もちろんです！当社のサービスには [機能1]、[機能2]、および [機能3] が含まれており、お客様の特定のニーズに応じて設計されています。これらの機能は非常に有益だと感じている現在のお客様からも優れたフィードバックをいただいています。\n",
        "[ユーザー] : うーん、有望ですね。でもまだ自分に合っているかどうか分かりません。\n",
        "[オペレーター] : お気持ち理解しています。迷われているのは分かります。お客様の決断をサポートするために、30日間の返金保証もご用意しています。これにより、1か月間はリスクを取らずにサービスを試していただけます。期待通りでない場合は、全額返金いたします。お気軽にお試しください。\n",
        "[ユーザー]: それは寛大な提案ですね。保証について感謝します。\n",
        "\"\"\"\n",
        "cls_embedding, tokenized_output, elapsed_time, encoded_input_corpus = get_cls_embedding_and_tokenized_output(input_corpus)\n",
        "\n",
        "print(\"CLS embedding's first 20 elements:\", cls_embedding.squeeze()[:20].tolist())\n",
        "print(\"CLS embedding size:\", cls_embedding.size())\n",
        "print(\"Inference time:\", elapsed_time)\n",
        "print(\"Input corpus: \", input_corpus)\n",
        "print(\"Tokenized output:\", tokenized_output)\n",
        "print(\"Tokenized output length:\", len(tokenized_output))\n",
        "# Print encoded_input_corpus properties\n",
        "print(\"Input IDs:\", encoded_input_corpus[\"input_ids\"])\n",
        "print(\"Attention Mask:\", encoded_input_corpus[\"attention_mask\"])\n",
        "# Compare with traditional BERT, there is no property \"token_type_ids\" so remove this code below\n",
        "#print(\"Token Type IDs:\", encoded_input_corpus[\"token_type_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2tk6PY6vORx",
        "outputId": "1d7849f9-a755-45e8-f23d-4b084b3b5ea7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "CLS embedding's first 20 elements: [-0.0028167073614895344, 0.0021248492412269115, -0.024699775502085686, -0.002365714404731989, 0.0037521759513765574, -0.006141373887658119, -0.004686365369707346, -0.004732964560389519, 0.0030468287877738476, -0.005801038350909948, 0.0012040735455229878, -0.016895325854420662, 0.007783760782331228, 0.0011932412162423134, -0.006864236202090979, -0.0020442288368940353, 0.0051956032402813435, -0.004687649663537741, 0.014514335431158543, -0.014260312542319298]\n",
            "CLS embedding size: torch.Size([1, 768])\n",
            "Inference time: 0.024875402450561523\n",
            "Input corpus:  \n",
            "[オペレーター] : こんにちは！お電話ありがとうございます。私の名前はエミリーで、今日はお手伝いできて嬉しいです。どのようにお手伝いできますか？\n",
            "[ユーザー] : こんにちは、エミリー。貴社のサービスに興味がありますが、それが私にとって適しているかどうか分かりません。\n",
            "[オペレーター] : 完全に理解していますし、興味を持っていただきありがとうございます！様々なニーズに対応した幅広いサービスを提供しています。サービスに関してもう少し教えていただけますか？\n",
            "[ユーザー] : そうですね、信頼性があり、手頃な価格のものが必要です。過去にサービスで悪い経験をしました。\n",
            "[オペレーター] : 了解しました。当社はすべてのサービスで信頼性と手頃な価格を重視しています。パッケージについての詳細を共有させていただきます。現在、新規顧客向けに割引料金が適用される特別なプロモーションを実施しています。これにより、高品質なサービスを手頃な価格でご利用いただけます。\n",
            "[ユーザー] : 興味深いですね。サービスの具体的な機能について教えていただけますか？\n",
            "[オペレーター] : もちろんです！当社のサービスには [機能1]、[機能2]、および [機能3] が含まれており、お客様の特定のニーズに応じて設計されています。これらの機能は非常に有益だと感じている現在のお客様からも優れたフィードバックをいただいています。\n",
            "[ユーザー] : うーん、有望ですね。でもまだ自分に合っているかどうか分かりません。\n",
            "[オペレーター] : お気持ち理解しています。迷われているのは分かります。お客様の決断をサポートするために、30日間の返金保証もご用意しています。これにより、1か月間はリスクを取らずにサービスを試していただけます。期待通りでない場合は、全額返金いたします。お気軽にお試しください。\n",
            "[ユーザー]: それは寛大な提案ですね。保証について感謝します。\n",
            "\n",
            "Tokenized output: ['[CLS]', '[', 'オペ', '##レーター', ']', ':', 'こん', '##にち', '##は', '!', 'お', '電話', 'ありがとう', 'ござい', 'ます', '。', '私', 'の', '名前', 'は', 'エミリー', 'で', '、', '今日', 'は', 'お', '##手', '##伝', '##い', 'でき', 'て', '嬉', '##しい', 'です', '。', 'どの', 'よう', 'に', 'お', '##手', '##伝', '##い', 'でき', 'ます', 'か', '?', '[', 'ユーザー', ']', ':', 'こん', '##にち', '##は', '、', 'エミリー', '。', '貴', '##社', 'の', 'サービス', 'に', '興味', 'が', 'あり', 'ます', 'が', '、', 'それ', 'が', '私', 'にとって', '適し', 'て', 'いる', 'か', 'どう', '##か', '分かり', 'ませ', 'ん', '。', '[', 'オペ', '##レーター', ']', ':', '完全', 'に', '理解', 'し', 'て', 'い', 'ます', 'し', '、', '興味', 'を', '持っ', 'て', 'いただ', '##き', 'ありがとう', 'ござい', 'ます', '!', '様々', 'な', 'ニーズ', 'に', '対応', 'し', 'た', '幅広い', 'サービス', 'を', '提供', 'し', 'て', 'い', 'ます', '。', 'サービス', 'に関して', 'もう少し', '教え', 'て', 'いただ', '##け', 'ます', 'か', '?', '[', 'ユーザー', ']', ':', 'そう', 'です', 'ね', '、', '信頼', '性', 'が', 'あり', '、', '手', '##頃', 'な', '価格', 'の', 'もの', 'が', '必要', 'です', '。', '過去', 'に', 'サービス', 'で', '悪い', '経験', 'を', 'し', 'まし', 'た', '。', '[', 'オペ', '##レーター', ']', ':', '了解', 'し', 'まし', 'た', '。', '当社', 'は', 'すべて', 'の', 'サービス', 'で', '信頼', '性', 'と', '手', '##頃', 'な', '価格', 'を', '重視', 'し', 'て', 'い', 'ます', '。', 'パッケージ', 'について', 'の', '詳細', 'を', '共有', 'さ', 'せ', 'て', 'いただ', '##き', 'ます', '。', '現在', '、', '新規', '顧客', '向け', 'に', '割引', '料金', 'が', '適用', 'さ', 'れる', '特別', 'な', 'プロモーション', 'を', '実施', 'し', 'て', 'い', 'ます', '。', 'これ', 'により', '、', '高', '品質', 'な', 'サービス', 'を', '手', '##頃', 'な', '価格', 'で', 'ご', '利用', 'いただ', '##け', 'ます', '。', '[', 'ユーザー', ']', ':', '興味深い', 'です', 'ね', '。', 'サービス', 'の', '具体', '的', 'な', '機能', 'について', '教え', 'て', 'いただ', '##け', 'ます', 'か', '?', '[', 'オペ', '##レーター', ']', ':', 'もちろん', 'です', '!', '当社', 'の', 'サービス', 'に', 'は', '[', '機能', '1', ']', '##、', '##[', '機能', '2', ']', '##、', 'および', '[', '機能', '3', ']', 'が', '含ま', 'れ', 'て', 'おり', '、', 'お客', '##様', 'の', '特定', 'の', 'ニーズ', 'に', '応じ', 'て', '設計', 'さ', 'れ', 'て', 'い', 'ます', '。', 'これら', 'の', '機能', 'は', '非常', 'に', '有', '##益', 'だ', 'と', '感じ', 'て', 'いる', '現在', 'の', 'お客', '##様', 'から', 'も', '優れ', 'た', 'フィードバック', 'を', 'いただ', '##い', 'て', 'い', 'ます', '。', '[', 'ユーザー', ']', ':', 'う', '##ー', '##ん', '、', '有望', 'です', 'ね', '。', 'でも', 'まだ', '自分', 'に', '合っ', 'て', 'いる', 'か', 'どう', '##か', '分かり', 'ませ', 'ん', '。', '[', 'オペ', '##レーター', ']', ':', 'お', '気持ち', '理解', 'し', 'て', 'い', 'ます', '。', '迷', '##わ', 'れ', 'て', 'いる', 'の', 'は', '分かり', 'ます', '。', 'お客', '##様', 'の', '決断', 'を', 'サポート', 'する', 'ため', 'に', '、', '30', '日間', 'の', '返', '##金', '保証', 'も', 'ご', '用意', 'し', 'て', 'い', 'ます', '。', 'これ', 'により', '、', '1', 'か月', '間', 'は', 'リスク', 'を', '取ら', 'ず', 'に', 'サービス', 'を', '試し', 'て', 'いただ', '##け', 'ます', '。', '期待', '通り', 'で', 'ない', '場合', 'は', '、', '全額', '返', '##金', 'いた', '##し', 'ます', '。', 'お', '気', '##軽', 'に', 'お', '試し', 'ください', '。', '[', 'ユーザー', ']', '##:', 'それ', 'は', '寛', '##大', 'な', '提案', 'です', 'ね', '。', '保証', 'について', '感謝', 'し', 'ます', '。', '[SEP]']\n",
            "Tokenized output length: 485\n",
            "Input IDs: tensor([[    2,  4314,  3454,  4384,  4118,   266, 10350, 25746, 28450,   679,\n",
            "            73,  2568, 21670, 27378,  2610,     8,  1325,     5,  1381,     9,\n",
            "         25704,    12,     6,  3246,     9,    73, 28608, 28925, 28457,   203,\n",
            "            16, 18607,   485,  2992,     8,  3219,   124,     7,    73, 28608,\n",
            "         28925, 28457,   203,  2610,    29,  2935,  4314,  4502,  4118,   266,\n",
            "         10350, 25746, 28450,     6, 25704,     8,  1685, 28609,     5,  1645,\n",
            "             7,  4878,    14,   130,  2610,    14,     6,   218,    14,  1325,\n",
            "          2032,  8149,    16,    33,    29,  1704, 28470, 14604,  6769,  1058,\n",
            "             8,  4314,  3454,  4384,  4118,   266,  1554,     7,  3426,    15,\n",
            "            16,    21,  2610,    15,     6,  4878,    11,  1330,    16, 23466,\n",
            "         28512, 21670, 27378,  2610,   679,  1400,    18, 10948,     7,  1277,\n",
            "            15,    10,  7771,  1645,    11,  1530,    15,    16,    21,  2610,\n",
            "             8,  1645,  2174, 26234,  4185,    16, 23466, 28517,  2610,    29,\n",
            "          2935,  4314,  4502,  4118,   266,  1778,  2992,  1852,     6,  4855,\n",
            "           245,    14,   130,     6,   319, 29175,    18,  3225,     5,   120,\n",
            "            14,   727,  2992,     8,  2147,     7,  1645,    12,  6981,  1903,\n",
            "            11,    15,  3913,    10,     8,  4314,  3454,  4384,  4118,   266,\n",
            "         21167,    15,  3913,    10,     8, 12703,     9,  1520,     5,  1645,\n",
            "            12,  4855,   245,    13,   319, 29175,    18,  3225,    11,  4789,\n",
            "            15,    16,    21,  2610,     8,  6756,   362,     5,  2912,    11,\n",
            "          5585,    26,   191,    16, 23466, 28512,  2610,     8,   265,     6,\n",
            "          5493,  8019,  1100,     7, 11365,  5073,    14,  3450,    26,    62,\n",
            "          1403,    18,  8682,    11,  1278,    15,    16,    21,  2610,     8,\n",
            "           171,   225,     6,   107,  7471,    18,  1645,    11,   319, 29175,\n",
            "            18,  3225,    12,   802,   666, 23466, 28517,  2610,     8,  4314,\n",
            "          4502,  4118,   266, 24241,  2992,  1852,     8,  1645,     5,  4163,\n",
            "            81,    18,  1197,   362,  4185,    16, 23466, 28517,  2610,    29,\n",
            "          2935,  4314,  3454,  4384,  4118,   266,  8871,  2992,   679, 12703,\n",
            "             5,  1645,     7,     9,  4314,  1197,    17,  4118, 28445, 29911,\n",
            "          1197,    25,  4118, 28445,   502,  4314,  1197,    48,  4118,    14,\n",
            "          1610,    20,    16,   206,     6, 23219, 28857,     5,  2255,     5,\n",
            "         10948,     7,  3144,    16,  1402,    26,    20,    16,    21,  2610,\n",
            "             8,   875,     5,  1197,     9,  1565,     7,   355, 29662,    75,\n",
            "            13,  3415,    16,    33,   265,     5, 23219, 28857,    40,    28,\n",
            "          4003,    10, 23510,    11, 23466, 28457,    16,    21,  2610,     8,\n",
            "          4314,  4502,  4118,   266,   205, 28451, 28575,     6, 20340,  2992,\n",
            "          1852,     8,   962,  2941,  1040,     7, 11384,    16,    33,    29,\n",
            "          1704, 28470, 14604,  6769,  1058,     8,  4314,  3454,  4384,  4118,\n",
            "           266,    73,  8415,  3426,    15,    16,    21,  2610,     8,  6566,\n",
            "         28544,    20,    16,    33,     5,     9, 14604,  2610,     8, 23219,\n",
            "         28857,     5, 10725,    11,  4283,    34,    82,     7,     6,   525,\n",
            "          3632,     5,  2434, 28790,  7591,    28,   802,  4060,    15,    16,\n",
            "            21,  2610,     8,   171,   225,     6,    17,  3521,   284,     9,\n",
            "          7215,    11,  6331,   255,     7,  1645,    11, 21496,    16, 23466,\n",
            "         28517,  2610,     8,  3252,   939,    12,    80,   344,     9,     6,\n",
            "         17763,  2434, 28790,  6785, 28454,  2610,     8,    73,   704, 29505,\n",
            "             7,    73, 21496, 16546,     8,  4314,  4502,  4118, 28811,   218,\n",
            "             9,  2488, 28500,    18,  2820,  2992,  1852,     8,  7591,   362,\n",
            "          8906,    15,  2610,     8,     3]], device='cuda:0')\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}