{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPu93DRdGIqBn8n4KPKmCH4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LienNguyen2912/Japanese-BERT-Model/blob/main/TohokuBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary libraries"
      ],
      "metadata": {
        "id": "wSkvHGzPByA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U0Nd7vhBdee",
        "outputId": "ad39387c-1e51-41cc-daaa-cd00ed24e5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bTm1gGWCV4Q",
        "outputId": "d56da3da-5bf3-4511-a497-5024c4738626"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5bmZxwWNJSV",
        "outputId": "02cc4ba7-956e-479a-e0b8-6b640812fd73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fugashi\n",
            "  Downloading fugashi-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (600 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m600.9/600.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fugashi\n",
            "Successfully installed fugashi-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mecab-python3\n",
        "!pip install unidic-lite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqb7gdNkNS1q",
        "outputId": "959aa236-3523-4e4f-d07e-d2b1b103010f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/581.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/581.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m532.5/581.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.7/581.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-1.0.8\n",
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=451a241dddf162d196f043082dea365f42568f38b23b58c997aea03eff891b09\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLS1QWwTK7pH",
        "outputId": "26fc6f11-f5ab-47d9-bfd3-f4d12d3fda69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 23 03:14:15 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P0    30W /  70W |   2241MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "#from transformers import BertTokenizer, BertModel\n",
        "from transformers import BertJapaneseTokenizer, BertModel\n",
        "# Load the Tohoku's Japanese BERT model\n",
        "model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print (device)\n",
        "\n",
        "# Define a function to classify a text and get CLS's embedding\n",
        "def get_cls_embedding_and_tokenized_output(input_corpus):\n",
        "    # Tokenize input and get token outputs\n",
        "    encoded_input_corpus  = tokenizer(input_corpus, return_tensors=\"pt\", truncation=True)\n",
        "    encoded_input_corpus = {key: val.to(device) for key, val in encoded_input_corpus.items()}\n",
        "\n",
        "    # Run the model and get CLS's embedding\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        output  = model(**encoded_input_corpus )\n",
        "        cls_embedding = output.last_hidden_state[:, 0]\n",
        "        # Convert the tokenized output to a list of tokens\n",
        "        tokenized_output = tokenizer.convert_ids_to_tokens(encoded_input_corpus[\"input_ids\"].squeeze().cpu().numpy())\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return cls_embedding, tokenized_output, elapsed_time, encoded_input_corpus\n",
        "\n",
        "#input_corpus = \"これは日本語のテキストのコープスです。\"\n",
        "input_corpus = \"\"\"\n",
        "[オペレーター] : こんにちは！お電話ありがとうございます。私の名前はエミリーで、今日はお手伝いできて嬉しいです。どのようにお手伝いできますか？\n",
        "[ユーザー] : こんにちは、エミリー。貴社のサービスに興味がありますが、それが私にとって適しているかどうか分かりません。\n",
        "[オペレーター] : 完全に理解していますし、興味を持っていただきありがとうございます！様々なニーズに対応した幅広いサービスを提供しています。サービスに関してもう少し教えていただけますか？\n",
        "[ユーザー] : そうですね、信頼性があり、手頃な価格のものが必要です。過去にサービスで悪い経験をしました。\n",
        "[オペレーター] : 了解しました。当社はすべてのサービスで信頼性と手頃な価格を重視しています。パッケージについての詳細を共有させていただきます。現在、新規顧客向けに割引料金が適用される特別なプロモーションを実施しています。これにより、高品質なサービスを手頃な価格でご利用いただけます。\n",
        "[ユーザー] : 興味深いですね。サービスの具体的な機能について教えていただけますか？\n",
        "[オペレーター] : もちろんです！当社のサービスには [機能1]、[機能2]、および [機能3] が含まれており、お客様の特定のニーズに応じて設計されています。これらの機能は非常に有益だと感じている現在のお客様からも優れたフィードバックをいただいています。\n",
        "[ユーザー] : うーん、有望ですね。でもまだ自分に合っているかどうか分かりません。\n",
        "[オペレーター] : お気持ち理解しています。迷われているのは分かります。お客様の決断をサポートするために、30日間の返金保証もご用意しています。これにより、1か月間はリスクを取らずにサービスを試していただけます。期待通りでない場合は、全額返金いたします。お気軽にお試しください。\n",
        "[ユーザー]: それは寛大な提案ですね。保証について感謝します。\n",
        "\"\"\"\n",
        "cls_embedding, tokenized_output, elapsed_time, encoded_input_corpus = get_cls_embedding_and_tokenized_output(input_corpus)\n",
        "\n",
        "print(\"CLS embedding's first 20 elements:\", cls_embedding.squeeze()[:20].tolist())\n",
        "print(\"CLS embedding size:\", cls_embedding.size())\n",
        "print(\"Inference time:\", elapsed_time)\n",
        "print(\"Input corpus: \", input_corpus)\n",
        "print(\"Tokenized output:\", tokenized_output)\n",
        "print(\"Tokenized output length:\", len(tokenized_output))\n",
        "# Print encoded_input_corpus properties\n",
        "print(\"Input IDs:\", encoded_input_corpus[\"input_ids\"])\n",
        "print(\"Attention Mask:\", encoded_input_corpus[\"attention_mask\"])\n",
        "print(\"Token Type IDs:\", encoded_input_corpus[\"token_type_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r83MMiSWCalI",
        "outputId": "bd222ee4-6408-48cb-f10f-ff4522dc67a9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "CLS embedding's first 20 elements: [0.8804212808609009, -0.09973394125699997, -0.5113552808761597, 0.05032897740602493, -0.07233496010303497, -0.16651344299316406, -0.24157138168811798, -0.4871055781841278, -0.534979522228241, 0.09847936034202576, 0.7304293513298035, -0.2848810851573944, 0.6309994459152222, 0.1546383500099182, -0.32958075404167175, -0.4615398943424225, 0.23815754055976868, -0.22955204546451569, 0.17011003196239471, -0.05814847722649574]\n",
            "CLS embedding size: torch.Size([1, 768])\n",
            "Inference time: 0.051265716552734375\n",
            "Input corpus:  \n",
            "[オペレーター] : こんにちは！お電話ありがとうございます。私の名前はエミリーで、今日はお手伝いできて嬉しいです。どのようにお手伝いできますか？\n",
            "[ユーザー] : こんにちは、エミリー。貴社のサービスに興味がありますが、それが私にとって適しているかどうか分かりません。\n",
            "[オペレーター] : 完全に理解していますし、興味を持っていただきありがとうございます！様々なニーズに対応した幅広いサービスを提供しています。サービスに関してもう少し教えていただけますか？\n",
            "[ユーザー] : そうですね、信頼性があり、手頃な価格のものが必要です。過去にサービスで悪い経験をしました。\n",
            "[オペレーター] : 了解しました。当社はすべてのサービスで信頼性と手頃な価格を重視しています。パッケージについての詳細を共有させていただきます。現在、新規顧客向けに割引料金が適用される特別なプロモーションを実施しています。これにより、高品質なサービスを手頃な価格でご利用いただけます。\n",
            "[ユーザー] : 興味深いですね。サービスの具体的な機能について教えていただけますか？\n",
            "[オペレーター] : もちろんです！当社のサービスには [機能1]、[機能2]、および [機能3] が含まれており、お客様の特定のニーズに応じて設計されています。これらの機能は非常に有益だと感じている現在のお客様からも優れたフィードバックをいただいています。\n",
            "[ユーザー] : うーん、有望ですね。でもまだ自分に合っているかどうか分かりません。\n",
            "[オペレーター] : お気持ち理解しています。迷われているのは分かります。お客様の決断をサポートするために、30日間の返金保証もご用意しています。これにより、1か月間はリスクを取らずにサービスを試していただけます。期待通りでない場合は、全額返金いたします。お気軽にお試しください。\n",
            "[ユーザー]: それは寛大な提案ですね。保証について感謝します。\n",
            "\n",
            "Tokenized output: ['[CLS]', '[', 'オペレーター', ']', ':', 'こん', '##にち', '##は', '!', 'お', '電話', 'ありがとう', 'ござい', 'ます', '。', '私', 'の', '名前', 'は', 'エミ', '##リー', 'で', '、', '今日', 'は', 'お', '手伝い', 'でき', 'て', '嬉', '##しい', 'です', '。', 'どの', 'よう', 'に', 'お', '手伝い', 'でき', 'ます', 'か', '?', '[', 'ユーザー', ']', ':', 'こん', '##にち', '##は', '、', 'エミ', '##リー', '。', '貴', '##社', 'の', 'サービス', 'に', '興味', 'が', 'あり', 'ます', 'が', '、', 'それ', 'が', '私', 'に', 'とっ', 'て', '適し', 'て', 'いる', 'か', 'どう', 'か', '分かり', 'ませ', 'ん', '。', '[', 'オペレーター', ']', ':', '完全', 'に', '理解', 'し', 'て', 'い', 'ます', 'し', '、', '興味', 'を', '持っ', 'て', 'いただ', '##き', 'ありがとう', 'ござい', 'ます', '!', '様々', 'な', 'ニーズ', 'に', '対応', 'し', 'た', '幅広い', 'サービス', 'を', '提供', 'し', 'て', 'い', 'ます', '。', 'サービス', 'に', '関し', 'て', 'もう', '少し', '教え', 'て', 'いただ', '##け', 'ます', 'か', '?', '[', 'ユーザー', ']', ':', 'そう', 'です', 'ね', '、', '信頼', '性', 'が', 'あり', '、', '手', '##頃', 'な', '価格', 'の', 'もの', 'が', '必要', 'です', '。', '過去', 'に', 'サービス', 'で', '悪い', '経験', 'を', 'し', 'まし', 'た', '。', '[', 'オペレーター', ']', ':', '了', '##解', 'し', 'まし', 'た', '。', '当社', 'は', 'すべて', 'の', 'サービス', 'で', '信頼', '性', 'と', '手', '##頃', 'な', '価格', 'を', '重視', 'し', 'て', 'い', 'ます', '。', 'パッケージ', 'に', 'つい', 'て', 'の', '詳細', 'を', '共有', 'さ', 'せ', 'て', 'いただ', '##き', 'ます', '。', '現在', '、', '新規', '顧客', '向け', 'に', '割引', '料金', 'が', '適用', 'さ', 'れる', '特別', 'な', 'プロモーション', 'を', '実施', 'し', 'て', 'い', 'ます', '。', 'これ', 'に', 'より', '、', '高', '品質', 'な', 'サービス', 'を', '手', '##頃', 'な', '価格', 'で', 'ご', '利用', 'いただ', '##け', 'ます', '。', '[', 'ユーザー', ']', ':', '興味', '深い', 'です', 'ね', '。', 'サービス', 'の', '具体', '的', 'な', '機能', 'に', 'つい', 'て', '教え', 'て', 'いただ', '##け', 'ます', 'か', '?', '[', 'オペレーター', ']', ':', 'もちろん', 'です', '!', '当社', 'の', 'サービス', 'に', 'は', '[', '機能', '1', ']、', '##[', '機能', '2', ']、', 'および', '[', '機能', '3', ']', 'が', '含ま', 'れ', 'て', 'おり', '、', 'お', '客', '様', 'の', '特定', 'の', 'ニーズ', 'に', '応じ', 'て', '設計', 'さ', 'れ', 'て', 'い', 'ます', '。', 'これ', 'ら', 'の', '機能', 'は', '非常', 'に', '有', '##益', 'だ', 'と', '感じ', 'て', 'いる', '現在', 'の', 'お', '客', '様', 'から', 'も', '優れ', 'た', 'フィード', '##バック', 'を', 'いただ', '##い', 'て', 'い', 'ます', '。', '[', 'ユーザー', ']', ':', 'う', '##ー', '##ん', '、', '有望', 'です', 'ね', '。', 'で', 'も', 'まだ', '自分', 'に', '合っ', 'て', 'いる', 'か', 'どう', 'か', '分かり', 'ませ', 'ん', '。', '[', 'オペレーター', ']', ':', 'お', '気持ち', '理解', 'し', 'て', 'い', 'ます', '。', '迷', '##わ', 'れ', 'て', 'いる', 'の', 'は', '分かり', 'ます', '。', 'お', '客', '様', 'の', '決断', 'を', 'サポート', 'する', 'ため', 'に', '、', '30', '日間', 'の', '返', '##金', '保証', 'も', 'ご', '用意', 'し', 'て', 'い', 'ます', '。', 'これ', 'に', 'より', '、', '1', 'か月', '間', 'は', 'リスク', 'を', '取ら', 'ず', 'に', 'サービス', 'を', '試し', 'て', 'いただ', '##け', 'ます', '。', '期待', '通り', 'で', 'ない', '場合', 'は', '、', '全額', '返', '##金', 'いた', '##し', 'ます', '。', 'お', '気', '##軽', 'に', 'お', '試し', 'ください', '。', '[', 'ユーザー', ']', '##:', 'それ', 'は', '寛', '##大', 'な', '提案', 'です', 'ね', '。', '保証', 'に', 'つい', 'て', '感謝', 'し', 'ます', '。', '[SEP]']\n",
            "Tokenized output length: 497\n",
            "Input IDs: tensor([[    2,    74, 31986,    76,    41, 20376, 32192,  7362,    16,   428,\n",
            "         13803, 26504, 30714, 12995,   385,  4262,   464, 13009,   465, 19137,\n",
            "         12503,   457,   384, 15028,   465,   428, 23306, 12535,   456,  1714,\n",
            "         12612, 13037,   385, 14949, 12515,   461,   428, 23306, 12535, 12995,\n",
            "           429,    46,    74, 15862,    76,    41, 20376, 32192,  7362,   384,\n",
            "         19137, 12503,   385,  5773,  8821,   464, 13202,   461, 15385,   430,\n",
            "         12517, 12995,   430,   384, 12546,   430,  4262,   461, 13322,   456,\n",
            "         19757,   456, 12483,   429, 13500,   429, 24869, 16707,   501,   385,\n",
            "            74, 31986,    76,    41, 13220,   461, 14623,   441,   456,   422,\n",
            "         12995,   441,   384, 15385,   500, 13008,   456, 27184,  7137, 26504,\n",
            "         30714, 12995,    16, 13268,   460, 26276,   461, 13122,   441,   449,\n",
            "         19502, 13202,   500, 13188,   441,   456,   422, 12995,   385, 13202,\n",
            "           461, 13861,   456, 13782, 15066, 14813,   456, 27184,  7139, 12995,\n",
            "           429,    46,    74, 15862,    76,    41, 13325, 13037,   463,   384,\n",
            "         15403,  2183,   430, 12517,   384,  2391,  9275,   460, 14908,   464,\n",
            "         12518,   430, 12804, 13037,   385, 13434,   461, 13202,   457, 16076,\n",
            "         13536,   500,   441, 14347,   449,   385,    74, 31986,    76,    41,\n",
            "           652,  8270,   441, 14347,   449,   385, 21902,   465, 13301,   464,\n",
            "         13202,   457, 15403,  2183,   458,  2391,  9275,   460, 14908,   500,\n",
            "         16352,   441,   456,   422, 12995,   385, 16853,   461, 12584,   456,\n",
            "           464, 14266,   500, 16936,   439,   445,   456, 27184,  7137, 12995,\n",
            "           385, 12565,   384, 16290, 19035, 12994,   461, 21151, 16138,   430,\n",
            "         15085,   439, 12492, 13065,   460, 18125,   500, 13010,   441,   456,\n",
            "           422, 12995,   385, 12538,   461, 12505,   384,  6741, 18803,   460,\n",
            "         13202,   500,  2391,  9275,   460, 14908,   457,   438, 12743, 27184,\n",
            "          7139, 12995,   385,    74, 15862,    76,    41, 15385, 15729, 13037,\n",
            "           463,   385, 13202,   464, 16065,  4037,   460, 13058,   461, 12584,\n",
            "           456, 14813,   456, 27184,  7139, 12995,   429,    46,    74, 31986,\n",
            "            76,    41, 20289, 13037,    16, 21902,   464, 13202,   461,   465,\n",
            "            74, 13058,    32, 29050,  7781, 13058,    33, 29050, 12656,    74,\n",
            "         13058,    34,    76,   430, 13570,   494,   456, 12536,   384,   428,\n",
            "          1771,  3068,   464, 13843,   464, 26276,   461, 14680,   456, 13298,\n",
            "           439,   494,   456,   422, 12995,   385, 12538,   491,   464, 13058,\n",
            "           465, 13340,   461,  2807,  9312,   450,   458, 14200,   456, 12483,\n",
            "         12565,   464,   428,  1771,  3068, 12488,   484, 15232,   449, 29835,\n",
            "         21442,   500, 27184,  7228,   456,   422, 12995,   385,    74, 15862,\n",
            "            76,    41,   424,  7053,  7374,   384, 32605, 13037,   463,   385,\n",
            "           457,   484, 14464, 12731,   461, 25078,   456, 12483,   429, 13500,\n",
            "           429, 24869, 16707,   501,   385,    74, 31986,    76,    41,   428,\n",
            "         16716, 14623,   441,   456,   422, 12995,   385,  5997,  7175,   494,\n",
            "           456, 12483,   464,   465, 24869, 12995,   385,   428,  1771,  3068,\n",
            "           464, 21336,   500, 14718, 12484, 12495,   461,   384, 12616, 14933,\n",
            "           464,  5986,  7640, 18686,   484,   438, 15212,   441,   456,   422,\n",
            "         12995,   385, 12538,   461, 12505,   384,    32, 14646,  6354,   465,\n",
            "         17250,   500, 17704,   444,   461, 13202,   500, 30660,   456, 27184,\n",
            "          7139, 12995,   385, 15002, 12841,   457, 12494, 12579,   465,   384,\n",
            "         28325,  5986,  7640, 17132,  7173, 12995,   385,   428,  3235,  7316,\n",
            "           461,   428, 30660, 14281,   385,    74, 15862,    76,  8083, 12546,\n",
            "           465,  1796,  7276,   460, 14376, 13037,   463,   385, 18686,   461,\n",
            "         12584,   456, 17667,   441, 12995,   385,     3]], device='cuda:0')\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
            "Token Type IDs: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}